(9548, 2)
(2388, 2)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(9548, 2)
(2388, 2)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
(9548, 2)
(2388, 2)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Cloning https://huggingface.co/IAyoub/finetuning-bert-sentiment-reviews into local empty directory.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
(9548, 2)
(2388, 2)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ayoub/Documents/projet/simplon/certif/projet_chef_doeuvre/avis_nlp/api/model/finetuning-bert-sentiment-reviews is already a clone of https://huggingface.co/IAyoub/finetuning-bert-sentiment-reviews. Make sure you pull the latest changes with `repo.git_pull()`.
/home/ayoub/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
(9548, 2)
(2388, 2)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ayoub/Documents/projet/simplon/certif/projet_chef_doeuvre/avis_nlp/api/model/finetuning-bert-sentiment-reviews is already a clone of https://huggingface.co/IAyoub/finetuning-bert-sentiment-reviews. Make sure you pull the latest changes with `repo.git_pull()`.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ayoub/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
(9548, 2)
(2388, 2)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ayoub/Documents/projet/simplon/certif/projet_chef_doeuvre/avis_nlp/api/model/finetuning-bert-sentiment-reviews is already a clone of https://huggingface.co/IAyoub/finetuning-bert-sentiment-reviews. Make sure you pull the latest changes with `repo.git_pull()`.
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
/home/ayoub/anaconda3/lib/python3.9/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
/tmp/ipykernel_11295/1142868687.py:5: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate
  load_accuracy = load_metric("accuracy")
{'eval_loss': 0.942838191986084, 'eval_accuracy': 0.5841708542713567, 'eval_f1': 0.24590163934426232, 'eval_runtime': 95.5258, 'eval_samples_per_second': 24.998, 'eval_steps_per_second': 1.57, 'epoch': 0.02}
{'eval_loss': 0.8976850509643555, 'eval_accuracy': 0.5841708542713567, 'eval_f1': 0.24583663758921492, 'eval_runtime': 95.0174, 'eval_samples_per_second': 25.132, 'eval_steps_per_second': 1.579, 'epoch': 0.03}
{'eval_loss': 0.8637286424636841, 'eval_accuracy': 0.6273031825795645, 'eval_f1': 0.3721399281844516, 'eval_runtime': 95.2722, 'eval_samples_per_second': 25.065, 'eval_steps_per_second': 1.574, 'epoch': 0.05}
{'eval_loss': 0.8188291788101196, 'eval_accuracy': 0.6302345058626466, 'eval_f1': 0.49478399111089116, 'eval_runtime': 95.7786, 'eval_samples_per_second': 24.933, 'eval_steps_per_second': 1.566, 'epoch': 0.07}
{'eval_loss': 0.7746375203132629, 'eval_accuracy': 0.6687604690117253, 'eval_f1': 0.49210655666034553, 'eval_runtime': 95.6681, 'eval_samples_per_second': 24.961, 'eval_steps_per_second': 1.568, 'epoch': 0.08}
{'eval_loss': 0.7042945027351379, 'eval_accuracy': 0.7144053601340034, 'eval_f1': 0.5651859465312262, 'eval_runtime': 94.5108, 'eval_samples_per_second': 25.267, 'eval_steps_per_second': 1.587, 'epoch': 0.1}
{'eval_loss': 0.6213316321372986, 'eval_accuracy': 0.7797319932998324, 'eval_f1': 0.7105429797234478, 'eval_runtime': 95.006, 'eval_samples_per_second': 25.135, 'eval_steps_per_second': 1.579, 'epoch': 0.12}
{'eval_loss': 0.595786452293396, 'eval_accuracy': 0.7893634840871022, 'eval_f1': 0.7413640996265678, 'eval_runtime': 94.5222, 'eval_samples_per_second': 25.264, 'eval_steps_per_second': 1.587, 'epoch': 0.13}
{'eval_loss': 0.6222068071365356, 'eval_accuracy': 0.7784757118927973, 'eval_f1': 0.7071756921381741, 'eval_runtime': 94.4074, 'eval_samples_per_second': 25.295, 'eval_steps_per_second': 1.589, 'epoch': 0.15}
{'eval_loss': 0.5430665612220764, 'eval_accuracy': 0.7985762144053601, 'eval_f1': 0.7474726417051828, 'eval_runtime': 94.9001, 'eval_samples_per_second': 25.163, 'eval_steps_per_second': 1.581, 'epoch': 0.17}
{'eval_loss': 0.5389808416366577, 'eval_accuracy': 0.8031825795644891, 'eval_f1': 0.744162016834003, 'eval_runtime': 95.3186, 'eval_samples_per_second': 25.053, 'eval_steps_per_second': 1.574, 'epoch': 0.18}
{'eval_loss': 0.5494145750999451, 'eval_accuracy': 0.8044388609715243, 'eval_f1': 0.7439035722055429, 'eval_runtime': 94.1471, 'eval_samples_per_second': 25.365, 'eval_steps_per_second': 1.593, 'epoch': 0.2}
{'eval_loss': 0.5627800226211548, 'eval_accuracy': 0.7893634840871022, 'eval_f1': 0.764378992888188, 'eval_runtime': 95.1959, 'eval_samples_per_second': 25.085, 'eval_steps_per_second': 1.576, 'epoch': 0.22}
{'eval_loss': 0.49716445803642273, 'eval_accuracy': 0.8295644891122278, 'eval_f1': 0.787352228548604, 'eval_runtime': 94.9171, 'eval_samples_per_second': 25.159, 'eval_steps_per_second': 1.58, 'epoch': 0.23}
{'eval_loss': 0.49608609080314636, 'eval_accuracy': 0.8312395309882747, 'eval_f1': 0.7843292378166469, 'eval_runtime': 96.1988, 'eval_samples_per_second': 24.824, 'eval_steps_per_second': 1.559, 'epoch': 0.25}
{'eval_loss': 0.5009350776672363, 'eval_accuracy': 0.8299832495812395, 'eval_f1': 0.7837153393789893, 'eval_runtime': 127.04, 'eval_samples_per_second': 18.797, 'eval_steps_per_second': 1.181, 'epoch': 0.27}
{'eval_loss': 0.49408039450645447, 'eval_accuracy': 0.8136515912897823, 'eval_f1': 0.7858679123691487, 'eval_runtime': 106.4162, 'eval_samples_per_second': 22.44, 'eval_steps_per_second': 1.41, 'epoch': 0.28}
{'eval_loss': 0.4843883514404297, 'eval_accuracy': 0.8224455611390284, 'eval_f1': 0.7824796005941262, 'eval_runtime': 103.0128, 'eval_samples_per_second': 23.182, 'eval_steps_per_second': 1.456, 'epoch': 0.3}
{'eval_loss': 0.4514204263687134, 'eval_accuracy': 0.842964824120603, 'eval_f1': 0.8067861702235306, 'eval_runtime': 109.5662, 'eval_samples_per_second': 21.795, 'eval_steps_per_second': 1.369, 'epoch': 0.32}
{'eval_loss': 0.4660390615463257, 'eval_accuracy': 0.8270519262981575, 'eval_f1': 0.7962594594063613, 'eval_runtime': 106.6703, 'eval_samples_per_second': 22.387, 'eval_steps_per_second': 1.406, 'epoch': 0.34}
{'eval_loss': 0.4605478346347809, 'eval_accuracy': 0.8274706867671692, 'eval_f1': 0.7923927330555983, 'eval_runtime': 101.8677, 'eval_samples_per_second': 23.442, 'eval_steps_per_second': 1.472, 'epoch': 0.35}
{'eval_loss': 0.474787175655365, 'eval_accuracy': 0.8274706867671692, 'eval_f1': 0.7769151241596629, 'eval_runtime': 101.7455, 'eval_samples_per_second': 23.47, 'eval_steps_per_second': 1.474, 'epoch': 0.37}
{'eval_loss': 0.5102003812789917, 'eval_accuracy': 0.8174204355108877, 'eval_f1': 0.7654258991358166, 'eval_runtime': 104.2769, 'eval_samples_per_second': 22.901, 'eval_steps_per_second': 1.438, 'epoch': 0.39}
{'eval_loss': 0.45992133021354675, 'eval_accuracy': 0.8245393634840871, 'eval_f1': 0.7953760925938459, 'eval_runtime': 116.0178, 'eval_samples_per_second': 20.583, 'eval_steps_per_second': 1.293, 'epoch': 0.4}
{'eval_loss': 0.4376077651977539, 'eval_accuracy': 0.8396147403685092, 'eval_f1': 0.8097124276011839, 'eval_runtime': 110.6081, 'eval_samples_per_second': 21.59, 'eval_steps_per_second': 1.356, 'epoch': 0.42}
{'eval_loss': 0.46437734365463257, 'eval_accuracy': 0.8220268006700168, 'eval_f1': 0.7918980295211151, 'eval_runtime': 114.9756, 'eval_samples_per_second': 20.77, 'eval_steps_per_second': 1.305, 'epoch': 0.44}
{'eval_loss': 0.46353277564048767, 'eval_accuracy': 0.8396147403685092, 'eval_f1': 0.7980277526954934, 'eval_runtime': 101.5126, 'eval_samples_per_second': 23.524, 'eval_steps_per_second': 1.478, 'epoch': 0.45}
{'eval_loss': 0.43451908230781555, 'eval_accuracy': 0.8421273031825796, 'eval_f1': 0.8070482121821666, 'eval_runtime': 98.83, 'eval_samples_per_second': 24.163, 'eval_steps_per_second': 1.518, 'epoch': 0.47}
{'eval_loss': 0.4320039749145508, 'eval_accuracy': 0.8396147403685092, 'eval_f1': 0.804752542250344, 'eval_runtime': 101.8691, 'eval_samples_per_second': 23.442, 'eval_steps_per_second': 1.472, 'epoch': 0.49}
{'eval_loss': 0.42575010657310486, 'eval_accuracy': 0.8446398659966499, 'eval_f1': 0.8095456040535032, 'eval_runtime': 108.4519, 'eval_samples_per_second': 22.019, 'eval_steps_per_second': 1.383, 'epoch': 0.5}
{'eval_loss': 0.4571510851383209, 'eval_accuracy': 0.8375209380234506, 'eval_f1': 0.7926142810136249, 'eval_runtime': 100.4979, 'eval_samples_per_second': 23.762, 'eval_steps_per_second': 1.493, 'epoch': 0.52}
{'eval_loss': 0.4381304979324341, 'eval_accuracy': 0.8400335008375209, 'eval_f1': 0.7985503878451997, 'eval_runtime': 99.7478, 'eval_samples_per_second': 23.94, 'eval_steps_per_second': 1.504, 'epoch': 0.54}
{'eval_loss': 0.4257933497428894, 'eval_accuracy': 0.8467336683417085, 'eval_f1': 0.8139025984874722, 'eval_runtime': 97.144, 'eval_samples_per_second': 24.582, 'eval_steps_per_second': 1.544, 'epoch': 0.55}
{'eval_loss': 0.4264586567878723, 'eval_accuracy': 0.847571189279732, 'eval_f1': 0.8091668207919488, 'eval_runtime': 98.0793, 'eval_samples_per_second': 24.348, 'eval_steps_per_second': 1.529, 'epoch': 0.57}
{'eval_loss': 0.42474043369293213, 'eval_accuracy': 0.8496649916247906, 'eval_f1': 0.8148115450022004, 'eval_runtime': 107.4049, 'eval_samples_per_second': 22.234, 'eval_steps_per_second': 1.397, 'epoch': 0.59}
{'eval_loss': 0.4309312105178833, 'eval_accuracy': 0.8400335008375209, 'eval_f1': 0.8044524550717207, 'eval_runtime': 108.6513, 'eval_samples_per_second': 21.979, 'eval_steps_per_second': 1.381, 'epoch': 0.6}
{'eval_loss': 0.4268190562725067, 'eval_accuracy': 0.8417085427135679, 'eval_f1': 0.8050418137925522, 'eval_runtime': 113.3529, 'eval_samples_per_second': 21.067, 'eval_steps_per_second': 1.323, 'epoch': 0.62}
{'eval_loss': 0.437942236661911, 'eval_accuracy': 0.8408710217755444, 'eval_f1': 0.8028850792675065, 'eval_runtime': 102.5877, 'eval_samples_per_second': 23.278, 'eval_steps_per_second': 1.462, 'epoch': 0.64}
{'eval_loss': 0.4255653917789459, 'eval_accuracy': 0.8408710217755444, 'eval_f1': 0.8084261845875189, 'eval_runtime': 114.5964, 'eval_samples_per_second': 20.838, 'eval_steps_per_second': 1.309, 'epoch': 0.65}
{'eval_loss': 0.42870068550109863, 'eval_accuracy': 0.838358458961474, 'eval_f1': 0.8018516716584508, 'eval_runtime': 123.4901, 'eval_samples_per_second': 19.338, 'eval_steps_per_second': 1.215, 'epoch': 0.67}
{'eval_loss': 0.48591116070747375, 'eval_accuracy': 0.8316582914572864, 'eval_f1': 0.7874353498957866, 'eval_runtime': 159.4156, 'eval_samples_per_second': 14.98, 'eval_steps_per_second': 0.941, 'epoch': 0.69}
{'eval_loss': 0.43315935134887695, 'eval_accuracy': 0.838358458961474, 'eval_f1': 0.8058340898452049, 'eval_runtime': 158.641, 'eval_samples_per_second': 15.053, 'eval_steps_per_second': 0.946, 'epoch': 0.7}
{'eval_loss': 0.4310157001018524, 'eval_accuracy': 0.8454773869346733, 'eval_f1': 0.813404160600676, 'eval_runtime': 156.8611, 'eval_samples_per_second': 15.224, 'eval_steps_per_second': 0.956, 'epoch': 0.72}
{'eval_loss': 0.43313705921173096, 'eval_accuracy': 0.8450586264656617, 'eval_f1': 0.8084892176771129, 'eval_runtime': 156.0131, 'eval_samples_per_second': 15.306, 'eval_steps_per_second': 0.961, 'epoch': 0.74}
{'eval_loss': 0.43659746646881104, 'eval_accuracy': 0.8421273031825796, 'eval_f1': 0.8034104199021509, 'eval_runtime': 152.8261, 'eval_samples_per_second': 15.626, 'eval_steps_per_second': 0.982, 'epoch': 0.75}
{'eval_loss': 0.41381412744522095, 'eval_accuracy': 0.8446398659966499, 'eval_f1': 0.8108086789056937, 'eval_runtime': 161.2083, 'eval_samples_per_second': 14.813, 'eval_steps_per_second': 0.93, 'epoch': 0.77}
{'eval_loss': 0.41221314668655396, 'eval_accuracy': 0.8417085427135679, 'eval_f1': 0.8117898043459721, 'eval_runtime': 156.4703, 'eval_samples_per_second': 15.262, 'eval_steps_per_second': 0.959, 'epoch': 0.79}
{'eval_loss': 0.4266357421875, 'eval_accuracy': 0.8446398659966499, 'eval_f1': 0.8068973086336624, 'eval_runtime': 154.6815, 'eval_samples_per_second': 15.438, 'eval_steps_per_second': 0.97, 'epoch': 0.8}
{'eval_loss': 0.4223463237285614, 'eval_accuracy': 0.8366834170854272, 'eval_f1': 0.8044166032116092, 'eval_runtime': 157.495, 'eval_samples_per_second': 15.162, 'eval_steps_per_second': 0.952, 'epoch': 0.82}
{'loss': 0.5269, 'learning_rate': 1.4416527079843662e-05, 'epoch': 0.84}
{'eval_loss': 0.4213833808898926, 'eval_accuracy': 0.8408710217755444, 'eval_f1': 0.8082055564596567, 'eval_runtime': 152.7694, 'eval_samples_per_second': 15.631, 'eval_steps_per_second': 0.982, 'epoch': 0.84}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
{'eval_loss': 0.41518616676330566, 'eval_accuracy': 0.8438023450586265, 'eval_f1': 0.8107342210790486, 'eval_runtime': 155.5807, 'eval_samples_per_second': 15.349, 'eval_steps_per_second': 0.964, 'epoch': 0.85}
{'eval_loss': 0.43129873275756836, 'eval_accuracy': 0.8412897822445561, 'eval_f1': 0.8035780551295191, 'eval_runtime': 160.4533, 'eval_samples_per_second': 14.883, 'eval_steps_per_second': 0.935, 'epoch': 0.87}
{'eval_loss': 0.41779854893684387, 'eval_accuracy': 0.8446398659966499, 'eval_f1': 0.8090623465328841, 'eval_runtime': 154.2304, 'eval_samples_per_second': 15.483, 'eval_steps_per_second': 0.973, 'epoch': 0.89}
{'eval_loss': 0.41544708609580994, 'eval_accuracy': 0.8488274706867671, 'eval_f1': 0.817912279403829, 'eval_runtime': 158.2411, 'eval_samples_per_second': 15.091, 'eval_steps_per_second': 0.948, 'epoch': 0.9}
{'eval_loss': 0.40986427664756775, 'eval_accuracy': 0.8463149078726968, 'eval_f1': 0.8123279368885877, 'eval_runtime': 155.4753, 'eval_samples_per_second': 15.359, 'eval_steps_per_second': 0.965, 'epoch': 0.92}
{'eval_loss': 0.41640812158584595, 'eval_accuracy': 0.8438023450586265, 'eval_f1': 0.8083570697357428, 'eval_runtime': 156.7136, 'eval_samples_per_second': 15.238, 'eval_steps_per_second': 0.957, 'epoch': 0.94}
{'eval_loss': 0.45980849862098694, 'eval_accuracy': 0.8371021775544388, 'eval_f1': 0.7978920315252555, 'eval_runtime': 151.3545, 'eval_samples_per_second': 15.778, 'eval_steps_per_second': 0.991, 'epoch': 0.95}
{'eval_loss': 0.47534507513046265, 'eval_accuracy': 0.8266331658291457, 'eval_f1': 0.780072026090458, 'eval_runtime': 154.93, 'eval_samples_per_second': 15.413, 'eval_steps_per_second': 0.968, 'epoch': 0.97}
{'eval_loss': 0.41434013843536377, 'eval_accuracy': 0.8345896147403685, 'eval_f1': 0.8035514637049812, 'eval_runtime': 153.869, 'eval_samples_per_second': 15.52, 'eval_steps_per_second': 0.975, 'epoch': 0.99}
{'train_runtime': 8674.9686, 'train_samples_per_second': 3.302, 'train_steps_per_second': 0.206, 'train_loss': 0.5114926936262745, 'epoch': 0.99}
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To https://huggingface.co/IAyoub/finetuning-bert-sentiment-reviews
   7784f1f..ccecc96  main -> main
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
To https://huggingface.co/IAyoub/finetuning-bert-sentiment-reviews
   ccecc96..e413609  main -> main
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...
To disable this warning, you can either:
	- Avoid using `tokenizers` before the fork if possible
	- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)
Xformers is not installed correctly. If you want to use memorry_efficient_attention to accelerate training use the following command to install Xformers
